{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["\n","# UofT DSI-CARTE ML Bootcamp\n","#### Date\n","#### Fairness and Equity - Lab 1, Day 4\n","#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya, Shehnaz Islam\n","##### Lab author: Nakul Upadhya\n","\n","As decision-making increasingly relies on artificial intelligence, the issue of fairness and equity in machine learning is rapidly becoming a larger concern. In this lab, we will introduce various metrics that help analyze the biases of our models, as well as techniques that can help mitigate these discrepancies.\n","\n","The main packages we will be using in this lab is `fairlearn` [1] and `shap` [4] along with all the other packages we have previously used.\n","\n"],"metadata":{"id":"bQV7JO4uoIfk"}},{"cell_type":"code","source":["## Install Packages\n","!pip install fairlearn\n","!pip install shap\n","!pip install xgboost\n","\n","## Import packages\n","import numpy as np\n","import pandas as pd\n","\n","# IGNORE THESE LINES. They just turn off some annoying messages associated with Fairlearn\n","pd.options.mode.chained_assignment = None\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"],"metadata":{"id":"ONptgKWToJkj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setup\n","For this lab, we will be using the Diabetes 130-Hospitals Dataset [2]. This dataset consists of 10 years worth of clinical care data at 130 US hospitals and integrated delivery networks. Each record represents the admission record for a patient diagnosed with diabetes whose stay lasted between one to fourteen days. The features describing each encounter include demographics, diagnoses, diabetic medications, number of visits in the year preceding the encounter, and payer information, as well as whether the patient was readmitted after release, and whether the readmission occurred within 30 days of the release [2].\n","\n","One common task on this dataset is predicting whether or not a patient will be re-admitted to the hospital.\n","\n","\n","Lets first read all the data in. We will also subset the data and only work with a few columns."],"metadata":{"id":"ocEDl1_giEte"}},{"cell_type":"code","source":["from fairlearn.datasets import fetch_diabetes_hospital\n","\n","diabetes_information = fetch_diabetes_hospital() # Get dataset\n","df = diabetes_information['data'] # Access the data\n","df = df.dropna() # Remove null data points\n","response = df['readmit_binary'] # Save the target variable\n","df = df.drop(columns = ['readmitted', 'readmit_binary']) # Remove the target columns\n","# We remove both `readmitted` and `readmit_binary` since they are effectively the same data\n","\n","df.info() # Print the DF Info"],"metadata":{"id":"dgj_12tp5h_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now lets pre-process the data a bit. We will:\n","* Scale any numerical columns to be between 0 and 1.\n","* Convert variables with only two values into 0 and 1\n","* Use one-hot-encoding on the non-binary categorical variables."],"metadata":{"id":"W2R16vEPRrWX"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","\n","df_list = []\n","\n","# Select numeric data and scale it\n","\n","numeric_columns = ['time_in_hospital','num_lab_procedures','num_procedures','num_medications', 'number_diagnoses']\n","numeric_data = df[numeric_columns] # select relevant columns\n","scaler = MinMaxScaler() # Create our scaler\n","numeric_data[numeric_columns] = scaler.fit_transform(numeric_data[numeric_columns]) # Fit our scaler\n","df_list.append(numeric_data) # Add the transformed data to a list\n","\n","# Select binary true/false columns and turn them into zeros and ones\n","true_false_columns = ['medicare', 'medicaid', 'had_emergency', 'had_inpatient_days','had_outpatient_days']\n","true_false_data = df[true_false_columns]\n","true_false_data.replace({'False': 0, 'True': 1}, inplace=True) # replace with numbers\n","true_false_data = true_false_data.astype(int) # Convert from categorical to numeric\n","df_list.append(true_false_data)\n","\n","# One hot encoding of the categorical columns\n","categorical_columns = ['race','gender','age','max_glu_serum','A1Cresult',\n","                       'insulin','change','diabetesMed']\n","categorical_data = df[categorical_columns]\n","encoder = OneHotEncoder(sparse_output=False) # Create our One-Hot Encoder\n","encoded_data = encoder.fit_transform(categorical_data) # Transforme the data\n","\n","# Rename the resultant columns so the names make sense\n","end_features = []\n","for feature, categories in zip(categorical_columns, encoder.categories_):\n","  end_features = end_features + [f\"{feature}_{category}\" for category in categories]\n","\n","encoded_data = pd.DataFrame(encoded_data, columns = end_features)\n","df_list.append(encoded_data)\n","\n","# Join all the data back together\n","X = pd.concat(df_list, axis=1)"],"metadata":{"id":"VWZzkszDRqjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets also split this into a train and test set."],"metadata":{"id":"npFKbD2MY0FS"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, response, test_size=0.3, random_state=42)\n","print(f\"There are {len(y_train)} training points and {len(y_test)} testing_points\")"],"metadata":{"id":"z2HnpHvOY3eX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Explainability\n","\n","One way we can use machine learning is to explore potential systemic issues present in the data. For this task, we can use interpretable models such as linear/logistic regression and decision trees.\n","\n","Lets try fitting a 3 layer decision tree on our model and print out the testing accuracy and what features are used.\n","\n"],"metadata":{"id":"KZxtRAjiAZJP"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","from sklearn.metrics import accuracy_score\n","import graphviz # Package containing visualization tools\n","\n","tree_clf = DecisionTreeClassifier(max_depth = 3)\n","\n","tree_clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_test, tree_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"],"metadata":{"id":"q7fcL9F4Wcmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["export_graphviz(tree_clf, out_file=\"mytree.dot\", feature_names=X_train.columns) # Save the visualization of the tree\n","with open(\"mytree.dot\") as f: # read the file back in\n","    dot_graph = f.read()\n","graphviz.Source(dot_graph) # display the tree"],"metadata":{"id":"hVdJYAmFZBOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Another common way to understand a model's decision is through looking at feature importance. By examining what features are being heavily used and how they impact the end predictions, we can understand patterns in the underlying system that may not have been apparent.\n","\n","**Your Turn**\n","1. In the cell below, we plot out the feature importance for our decision tree. Do the results align with the splits you noticed in the above tree?\n"],"metadata":{"id":"IUbVi6ndd7cx"}},{"cell_type":"code","source":["import plotly.express as px # Interactive Plotting Package\n","\n","tree_importances = pd.DataFrame()\n","tree_importances['feature'] = X_train.columns\n","tree_importances['tree_importance'] = tree_clf.feature_importances_\n","tree_importances.sort_values(by = 'tree_importance', inplace =True, ascending = False)\n","# Remove features without any importance\n","importances = tree_importances[tree_importances['tree_importance'] > 0]\n","px.bar(importances, x = 'feature', y = 'tree_importance')"],"metadata":{"id":"MRnZYVUmeM0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately the decision tree is not the ideal model for many tasks, and we may want some increased complexity and may switch to models such as XGBoost or SVM. However, we still may want to understand some of the decision mechanisms present.\n","\n","Since XGBoost uses hundreds or thousands or trees in its prediction, it is not reasonable to plot those trees out and examine each one of them. Luckily, XGBoost still provides feature importance. Lets train an XGBoost network and see how this works:\n"],"metadata":{"id":"0YwvSkUZXy1w"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","\n","xgb_clf = XGBClassifier()\n","xgb_clf.fit(X_train, y_train)\n","accuracy = accuracy_score(y_test, xgb_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")\n","importances = pd.DataFrame()\n","importances['feature'] = X_train.columns\n","importances['importance'] = xgb_clf.feature_importances_\n","importances.sort_values(by = 'importance', inplace =True, ascending = False)\n","# Remove features without any importance\n","importances = importances[importances['importance'] > 0]\n","px.bar(importances, x = 'feature', y = 'importance')"],"metadata":{"id":"cAC6R1vp9MLJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice how the XGBoost model used many more features than the decision tree model. However, the features that were important in the decision tree are also some of the more important features in the XGBoost model.\n","\n","Not all models can provide feature importance however. One example of this is SVM where it simply creates hyperplanes to seperate classes. So what do we do in this case? This is where *post-hoc interpretability* methods come into play. Post-hoc methods work y taking in a trained models, modifying the inputs, and examining how significantly the outputs changed.\n","\n","One such interpretability tool is SHAP (Shapely Additive Values). SHAP provides feature importance by using methods from game theory to estimate the contribution of each feature towards the final prediction.\n","\n","\n","SHAP can provide a sense of both local (explaining a single prediction) and global (explaining general prediction trends) interpretability.\n","\n","To start, we first need to train a model to explain. For this exercise, we will use SVM with a radial kernel"],"metadata":{"id":"bNoG_XGQWc8R"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","svc_clf = SVC(kernel = 'rbf', max_iter = 2000)\n","svc_clf.fit(X_train, y_train)\n","accuracy = accuracy_score(y_test, svc_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"],"metadata":{"id":"SLi3Fte7DNFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now to start our explanation process. We start off by first creating a summary of our dataset (this is to make SHAP run faster) and creating our explainer object.\n","\n","\n","\n"],"metadata":{"id":"7FKmTwqiDNLu"}},{"cell_type":"code","source":["import shap\n","# rather than use the whole training set to estimate expected values, we summarize with\n","# a set of weighted kmeans, each weighted by the number of points they represent.\n","# this helps everything run faster\n","X_train_summary = shap.kmeans(X_train, 7)\n","\n","# Create the shap explainer by passing in our model's predict function and\n","# the summarized training set\n","ex = shap.KernelExplainer(svc_clf.predict, X_train_summary, feature_names = X_train.columns)\n","\n","# We are also only going to look at 100 points (to make it easier to visualize)\n","X_test_subset = X_test.sample(100, random_state = 42).reset_index(drop=True)"],"metadata":{"id":"_4OPT_6KEGV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Local Explainability\n","\n","Let's first look into the local explainability provided by SHAP by examining what contributes to the predictions of the first datapoint in our testing subset."],"metadata":{"id":"wBe1Gvt-QeZ4"}},{"cell_type":"code","source":["shap.initjs()\n","first_datapoint = X_test_subset.iloc[0]\n","single_point_shap_values = ex.shap_values(first_datapoint)\n","shap.force_plot(ex.expected_value, single_point_shap_values, X_test_subset.iloc[0])"],"metadata":{"id":"iK7FNfz9H8uA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the plot above, feature values that increased the chance of the model predicting a readmission are in red and have arrows that point to the right (they provide a positive force) and feature values that detract from the probability of readmission are in blue and point to the left. The larger the arrow, the larger the contribution.\n","\n","**Your Turn**\n","\n","* What features seem to have the most negative impact to the end prediction of the data point you chose? What about the one with the most positive impact? *YOUR ANSWER HERE*\n","* Choose a different data point and see if you see any similarities in the features used and their impact towards the end prediction. *YOUR ANSWER HERE*\n"],"metadata":{"id":"j0WphK7PLWmm"}},{"cell_type":"markdown","source":["#### Global Explanability\n","We can get a sense of global interpretability from SHAP by examining trends in the SHAP values across the variable values. To do this, we can generate a summary plot of the calculated values. NOTE: This may take a while....."],"metadata":{"id":"MYfQcTgXMMI0"}},{"cell_type":"code","source":["shap.initjs()\n","shap_values = ex.shap_values(X_test_subset)\n","shap.summary_plot(shap_values, X_test_subset)"],"metadata":{"id":"hC06z6H9QwiB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The color of the point reflects the value of a given feature in a given data point. For example, a red point in `diabetesMed_Yes` means that the feature took a value of 1 (true) and a blue point means a value of 0 (false). The X-axis of this plot represents SHAP contribution (the estimated impact on the end model prediction). By examining the distribution of the feature values across the x-axis, we can find what features may"],"metadata":{"id":"9NvmzymoWiag"}},{"cell_type":"code","source":["abs_shap_values = np.abs(shap_values)\n","shap_sums = np.sum(abs_shap_values, axis=0)\n","importances = pd.DataFrame()\n","importances['feature'] = X_train.columns\n","importances['importance'] = shap_sums\n","importances.sort_values(by = 'importance', inplace =True, ascending = False)\n","importances = importances[importances['importance'] > 0]\n","px.bar(importances, x = 'feature', y = 'importance')"],"metadata":{"id":"aXfrs9GJK5-f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Your Turn**\n","\n","* What features seem important to the SVM model according to SHAP? *YOUR ANSWER HERE*\n","* Do these SHAP importances align with the XGBoost and Decision Tree Importances? *YOUR ANSWER HERE\n","\n","Now consider all three models and the feature importances found.\n","* Are there any pattern that make sense to you? *YOUR ANSWER HERE*\n","* Are there any patterns that seem concerning from an equity perspective? *YOUR ANSWER HERE*"],"metadata":{"id":"6vvC9-mfM1iM"}},{"cell_type":"markdown","source":["#### Notes about SHAP\n","SHAP is an incredibly powerful tool to understand what your model may be doing, ***however it is only an estimate***. The SHAP value calculations only examine your model's behavior and do not dive into the internals of the model, therefore these values should not be taken at face value. Additionally, as you may have noticed in the plots above, SHAP values do not reflect interacting effects between features, something that most models do in fact use. This extends to other post-hoc interpretability methods as well.\n","\n","As such, it is highly encouraged to use innately interpretable models whenever possible. For a more rigourous justification, please read Cynthia Rudin's paper on the subject [3] after the lab.\n","\n","Additionally, quoting the SHAP documentation [4]:\n","\n","\n","> Predictive machine learning models like XGBoost become even more powerful when paired with interpretability tools like SHAP. These tools identify the most informative relationships between the input features and the predicted outcome, which is useful for explaining what the model is doing, getting stakeholder buy-in, and diagnosing potential problems. It is tempting to take this analysis one step further and assume that interpretation tools can also identify what features decision makers should manipulate if they want to change outcomes in the future. However, in [this article](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html), we discuss how using predictive models to guide this kind of policy choice can often be misleading.\n","\n","> *Eleanor Dillon, Jacob LaRiviere, Scott Lundberg, Jonathan Roth, and Vasilis Syrgkanis from Microsoft.*\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"mIsp4G29aZxB"}},{"cell_type":"markdown","source":["\n","## Fairness\n","Now that we have understood a bit of what are model is looking at and in general how it handles the features, lets examine how fair and equitable our models performance is. Ideally, a fair model should perform identically across different sensitive identities.\n","\n","Lets examine the fairness of our XGBoost predictions\n","\n","\n"],"metadata":{"id":"maL8tP4Ouh_I"}},{"cell_type":"code","source":["y_pred = xgb_clf.predict(X_test) # Save the Test Predictions"],"metadata":{"id":"CGoOLPAZOCBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Measurement\n","The first step in ML fairness is to measure how fair the model is. For this, we will introduce two metrics.\n","\n","The first measure is *Demographic Parity*, or more accurately, the distance from demographic parity. Demographic parity is achieved when the probability of a certain prediction is not dependent on a point being in a sensitive group. This metric takes a range between 0 and 1 where 0 means we have achieved perfect demographic parity with respect to that feature.\n","\n","In general, a parity of under 20% is acceptable in many countries like the United States [1] to avoid legal problems, **however this should not be the goal as \"legally acceptable\" is not equal to \"fair\"**, especially in a high-stakes application such as healthcare (like the dataset we are working with). Read more about the 4/5ths fallacy in the [fairlearn documentation](https://fairlearn.org/v0.8/user_guide/assessment/common_fairness_metrics.html#the-four-fifths-rule-often-misapplied) [1] after the lab.\n","\n","**Your Turn**\n","* For this problem in particular, what do you think an acceptable parity difference would be? *YOUR ANSWER HERE*\n","* Run the cell below that evaluates the difference between asian individuals and non-asian individuals. Is this model fair with regards to this feature? *YOUR ANSWER HERE*\n","* Test your own sensitive feature by changing `feature_under_examination`.Try a few features to see if you can find one that exceeds the margin you decided. Were you able to find one? *YOUR ANSWER HERE*\n","\n","Tip: Get all features in our data used by running `X_test.columns`."],"metadata":{"id":"QpxsMDK3d8b_"}},{"cell_type":"code","source":["from fairlearn.metrics import demographic_parity_difference\n","feature_under_examination = 'race_Asian'\n","parity_difference = demographic_parity_difference(y_test,\n","                                    y_pred,\n","                                    sensitive_features=X_test[feature_under_examination])\n","parity_difference = np.round(parity_difference * 100, 3)\n","print(f\"The demographic parity difference for {feature_under_examination} is {parity_difference}%\")"],"metadata":{"id":"-0J_ur4lk3xS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next metric is Equalized Odds. If equalized odds are achieved, that means that the difference in true positive rates and true negative rates across classes is the same. Often times, we measure the maximum difference between these metrics across the classes. For example, if the TPR and TNR for men were 75% and 65% and the rates for women were 73% and 52%, we would report back 13%.\n","\n","Similar to demographic parity, we want this to be as low as possible. Additionally, this metric is also subject to the 4/5ths fallacy as well.\n","\n","**Your Turn**\n","* Between demographic parity and equalized odds, which is a harder criteria to achieve and why? *YOUR ANSWER HERE*\n","* For this problem in particular, what do you think an acceptable equalized odds difference would be? *YOUR ANSWER HERE*\n","* Run the cell below that evaluates the difference between asian individuals and non-asian individuals. Is this model fair with regards to this feature? *YOUR ANSWER HERE*\n","* Test your own sensitive feature by changing `feature_under_examination` in the cell below.Try a few features to see if you can find one that exceeds the margin you decided. Were you able to find one? *YOUR ANSWER HERE*\n","\n"],"metadata":{"id":"1SzhQV7WoNdW"}},{"cell_type":"code","source":["from fairlearn.metrics import equalized_odds_difference\n","feature_under_examination = 'race_Asian'\n","eo_difference = equalized_odds_difference(y_test,\n","                                    y_pred,\n","                                    sensitive_features=X_test[feature_under_examination])\n","eo_difference = np.round(eo_difference * 100, 3)\n","print(f\"The Equalized Odds difference for {feature_under_examination} is {eo_difference}%\")"],"metadata":{"id":"kmk1WHiRk4HT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The examples above only focus on identifying fairness across a single group, but most people do not have a single identity and we also need to examine the impacts of intersectionality (ex. white man vs. black man or woman with medicare vs. woman without medicare).\n","\n","**Your Turn**\n","\n","* Run the cell below that evaluates the equalized odds difference for Asian Women. Is this model fair with regards to the intersection of these features? *YOUR ANSWER HERE*\n","* Test your own sensitive features by changing `feature_1` and `feature_2`.Try a few features to see if you can find an intersection that exceeds the margin you decided. Were you able to find one? Make sure the meaning of the features are different (don't use two race features or two gender features).  *YOUR ANSWER HERE*"],"metadata":{"id":"HjUrGeQIr1-K"}},{"cell_type":"code","source":["feature_1 = 'race_Asian'\n","feature_2 = 'gender_Female'\n","eo_intersect_dif = equalized_odds_difference(y_test,\n","                                    y_pred,\n","                                    sensitive_features=X_test[[feature_1, feature_2]])\n","eo_intersect_dif = np.round(eo_intersect_dif * 100, 3)\n","print(f\"The Equalized Odds difference for {feature_1} and {feature_2} is {eo_intersect_dif}%\")"],"metadata":{"id":"Zr7wqyd7r1Ub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Mitigation\n","One way of mitigating unfairness is by adding constraints on the differences we mentioned above. This is done through the ExponentiatedGradient reduction where we re-weight samples during the training process until our metric (equalized odds or demographic parity) is below a set threshold in our training set.\n","\n","\n","Lets refit the model you chose with a Equalized Odds constraint with regards to asian individuals like before.\n","\n","**Your Turn**\n","* Change `maximum_difference` to be the difference you find acceptable\n","* Run the cell below and report back the equalized odds difference on the testing set. Did you achieve your fairness goal? If not, did the fairness improve? *YOUR ANSWER HERE*\n","\n","\n","NOTE: The training process may take a long time (5-15 minutes) as the model will be refitted multiple times. While you wait, I encourage you to read some of the resources linked in this notebook (ex. the 4/5ths fallacy article or any of the papers in the references).\n","\n"],"metadata":{"id":"ofN6cU52ujVP"}},{"cell_type":"code","source":["from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient\n","## Create the model\n","model = XGBClassifier()\n","\n","maximum_difference =  # change this to your acceptable difference\n","\n","feature_under_examination = 'race_Asian'\n","\n","reduction = ExponentiatedGradient( # Initialize the reduction mechanism\n","    model,\n","    EqualizedOdds(difference_bound = maximum_difference),\n","    eps = 1e-3\n",")\n","## THIS WILL TAKE A LONG TIME\n","reduction.fit(X_train, y_train, sensitive_features = X_train[feature_under_examination]) # Run the reduction mechanism\n","\n","y_pred_reduced = reduction.predict(X_test) # Make a prediction with the fair model\n","\n","eo_intersect_dif = equalized_odds_difference(y_test,\n","                                    y_pred_reduced,\n","                                    sensitive_features=X_test[feature_under_examination])\n","eo_intersect_dif = np.round(eo_intersect_dif * 100, 3)\n","print(f\"The Equalized Odds difference for {feature_under_examination} is {eo_intersect_dif}%\")"],"metadata":{"id":"tPG9b93W0XaB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extra Exploration\n","If you get done with the lab early, or want to further explore this topic, consider the following exercises:\n","\n","* Choose a different model than the one you trained above and conduct a study on the model behavior based on what you have learned above.\n","* Examine if the new model is focusing on similar or different features compared to your previous model\n","* Examine the fairness of your new model on a few features? Is it significantly different compared to your old model?\n","* Examine the fairness of your new model when looking at intersecting features and see how it compares to your old model."],"metadata":{"id":"HS73ABz3M4-t"}},{"cell_type":"code","source":["# Exploration Code here"],"metadata":{"id":"mi_qHTYZNAeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## References\n","1. Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., … Walker, K. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI. Retrieved from Microsoft website: https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/\n","2. Beata Strack, Jonathan Deshazo, Chris Gennings, Juan Luis Olmo Ortiz, Sebastian Ventura, Krzysztof Cios, and John Clore. Impact of hba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records. BioMed research international, 2014:781670, 04 2014. doi:10.1155/2014/781670.\n","3. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell 1, 206–215 (2019). https://doi.org/10.1038/s42256-019-0048-x\n","4. Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30 (pp. 4765–4774). Retrieved from http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n","\n","\n"],"metadata":{"id":"HorZkT-K4MOf"}}]}