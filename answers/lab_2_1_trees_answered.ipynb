{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0pPVB4-LHp"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2023/blob/main/lab_2_1_trees.ipynb)\n",
    "\n",
    "# UofT FASE ML Bootcamp\n",
    "#### Tuesday June 20, 2023\n",
    "#### Decision Trees & Random Forests - Lab 1, Day 2\n",
    "#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya, Shehnaz Islam\n",
    "##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca, edited by Jake Mosseri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3flbTck-LHu"
   },
   "source": [
    "So far, we've learned about nearest neighbors, support vector machines (SVM) and regression techniques. In this lab, we will be introducing *decision tree and forests*. We will introduce the notion of a decision tree, extend this to random forests, and then investigate some state-of-the-art tree-based methods for machine learning. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KPXh7nee-LHv",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:18:52.477323Z",
     "start_time": "2023-06-20T17:18:44.341346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (1.25.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from scikit-learn) (1.25.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from scikit-learn) (3.1.0)\r\n",
      "Requirement already satisfied: pandas in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (2.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from pandas) (1.25.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: xgboost in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (1.7.6)\r\n",
      "Requirement already satisfied: numpy in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from xgboost) (1.25.0)\r\n",
      "Requirement already satisfied: scipy in /Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages (from xgboost) (1.10.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Install the needed packages\n",
    "!pip install numpy # Matrix and Vector operations\n",
    "!pip install scikit-learn # ML Models\n",
    "!pip install pandas # Data Manipulation\n",
    "!pip install xgboost # More ML Models\n",
    "\n",
    "# Import numpy and sklearn\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7CedHdx-LHw"
   },
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are popular supervised learning methods used for classification and regression. The tree represents a series of simple decision rules that predict the target when the feature vector is passed through them. Decision trees are easy to understand, can be visualized nicely, require very little data preparation (e.g., we don't need to scale features), and the trained model can be explained easily to others post priori (as opposed to other *black box* methods that are difficult to communicate).\n",
    "\n",
    "###### Example\n",
    "Suppose you wanted to design a simple decision tree for whether (or not) you buy a used car. You might develop something like the following:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/decision-tree.gif?raw=1\" width=\"500\"/>\n",
    "\n",
    "**YOUR TURN:** Let's say you're browsing Kijiji and come across a used car that: has been road tested, has high mileage, and is a recent year/model.\n",
    "* According to your decision tree model, should you buy this car or not? **Yes**\n",
    "* Will you buy any cars that haven't been road tested (if you follow your model)? **No**\n",
    "\n",
    "Obviously this tree may not be ideal, depending on the situation. For example, you could have a road tested car of a recent year with 2,000,000 km's on it and the model is telling you to buy! (But, you probably shouldn't)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "litVF00HrKud"
   },
   "source": [
    "**Titanic Survivor Dataset**\n",
    "\n",
    "In this lab, we will be exploring the use of decision trees in the context of Kaggle's famous **Titanic dataset**. Each row in the data represents a passenger, detailing various characteristics about them (i.e., the features), and also details whether or not the passenger survived the disaster.\n",
    "\n",
    "Let's load the data and take a look at it.\n",
    "\n",
    "To get the data into a manageable format, we're going to use the [Pandas](https://pandas.pydata.org/) library, a popular library for data manipulation and analysis. While we won't be providing a full Pandas tutorial, we will provide some insight into key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mDl1JKwx-LHx",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:19:07.717483Z",
     "start_time": "2023-06-20T17:19:07.125567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "   pclass  survived                                             name     sex  \\\n0     1.0         1                    Allen, Miss. Elisabeth Walton  female   \n1     1.0         1                   Allison, Master. Hudson Trevor    male   \n2     1.0         0                     Allison, Miss. Helen Loraine  female   \n3     1.0         0             Allison, Mr. Hudson Joshua Creighton    male   \n4     1.0         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n\n       age  sibsp  parch  ticket      fare    cabin embarked  \n0  29.0000    0.0    0.0   24160  211.3375       B5        S  \n1   0.9167    1.0    2.0  113781  151.5500  C22 C26        S  \n2   2.0000    1.0    2.0  113781  151.5500  C22 C26        S  \n3  30.0000    1.0    2.0  113781  151.5500  C22 C26        S  \n4  25.0000    1.0    2.0  113781  151.5500  C22 C26        S  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pclass</th>\n      <th>survived</th>\n      <th>name</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>ticket</th>\n      <th>fare</th>\n      <th>cabin</th>\n      <th>embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>Allen, Miss. Elisabeth Walton</td>\n      <td>female</td>\n      <td>29.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>24160</td>\n      <td>211.3375</td>\n      <td>B5</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>Allison, Master. Hudson Trevor</td>\n      <td>male</td>\n      <td>0.9167</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>Allison, Miss. Helen Loraine</td>\n      <td>female</td>\n      <td>2.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>Allison, Mr. Hudson Joshua Creighton</td>\n      <td>male</td>\n      <td>30.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n      <td>female</td>\n      <td>25.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # import pandas to get access to dataframe operations\n",
    "from sklearn.datasets import fetch_openml # import function to retrieve relevant datasets\n",
    "\n",
    "full_data = fetch_openml(\"titanic\", version=1, as_frame=True) # Get all data and metadata\n",
    "data = full_data.frame # Extract the relevant data\n",
    "data.survived = pd.to_numeric(data['survived'])\n",
    "data.drop(['boat', 'body', 'home.dest'], axis=1, inplace=True) # Drop irrelevant columns\n",
    "data.head() # view the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ulk4gvl-LHz"
   },
   "source": [
    "The above cell used the `fetch_openml` function to pull in the Titanic survivor data. The `.head()` allows us to conveniently take a glance at the first 5 rows (along with the header).\n",
    "\n",
    "We can see that, along with the target 'Survived', we have a number of features including the passenger name, sex, age, fare, cabin, etc. We can do a bit of simple *exploratory data analysis* (EDA) to get a better feel for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yDz5AhGs-LHz",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:19:09.157320Z",
     "start_time": "2023-06-20T17:19:09.146475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passengers, features:  (1309, 11)\n",
      "Survived:  500 , Didn't:  809\n",
      "female:  466 , Male:  843\n",
      "\n",
      " Missing values by feature: \n",
      " pclass         0\n",
      "survived       0\n",
      "name           0\n",
      "sex            0\n",
      "age          263\n",
      "sibsp          0\n",
      "parch          0\n",
      "ticket         0\n",
      "fare           1\n",
      "cabin       1014\n",
      "embarked       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Passengers, features: \", data.shape)\n",
    "print (\"Survived: \", data[data[\"survived\"]==1].shape[0], \", Didn't: \", data[data[\"survived\"]==0].shape[0])\n",
    "print (\"female: \", data[data[\"sex\"]==\"female\"].shape[0], \", Male: \", data[data[\"sex\"]==\"male\"].shape[0])\n",
    "print (\"\\n Missing values by feature: \\n\", data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfTLCpGf-LH0"
   },
   "source": [
    "As you can see, we can use Pandas to conveniently summarize key aspects of the dataset such as the number of passengers, features, survived/didn't, and their gender. We are also able to identify the number of missing values per feature in the dataset.\n",
    "\n",
    "To accomplish this, we used Pandas flexible indexing capability. The syntax `data[data[col]==val]` allows us to return the subset of rows in `data` where column `col` takes on value `val`. Very powerful!\n",
    "\n",
    "As you may have suspected, the dataset we're using is actually a subset of the total Titanic data. In reality, there were actually 3,547 passengers while the data we're working with only concerns 1309 of them.\n",
    "\n",
    "**YOUR TURN:**\n",
    "Using similar syntax, answer the following questions about the data:\n",
    "* In the dataset, what is the passenger survival rate? **38%**\n",
    "* How many passengers paid more than $10 for fare? **817**\n",
    "* How many passengers had a passenger class (Pclass) of 3? **709**\n",
    "* With some discussion/exploration and try to determine what features might be the most relevant to passenger survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3qH9urfd-LH1",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:19:52.973919Z",
     "start_time": "2023-06-20T17:19:52.960585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger survival rate:                    0.38\n",
      "Passengers who paid more than $10 for fare: 817\n",
      "Passengers with a Pclass of 3:              709\n"
     ]
    }
   ],
   "source": [
    "## Your code here\n",
    "print(f'Passenger survival rate:                    {data[data[\"survived\"]==1].shape[0]/data.shape[0]:.2f}')\n",
    "print(f'Passengers who paid more than $10 for fare: {data[data[\"fare\"]>10].shape[0]}')\n",
    "print(f'Passengers with a Pclass of 3:              {data[data[\"pclass\"]==3].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB5y8KEa-LH2"
   },
   "source": [
    "##### Data Preparation: Categorical -> Numerical Mapping\n",
    "\n",
    "Before we can fit sklearn decision trees to our data, we first need to convert all of the categorical variables (e.g., gender) numerical values - this is called *encoding*. In previous labs, we dealt with datasets that were pre-prepared; now things are getting a little more realistic! Categoricals with unique values (like name and ticket #) can be removed from the dataset entirely as we don't suspect they will contribute to the model.\n",
    "\n",
    "We can do the required preparation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0WjofYtW-LH2",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:19:55.093067Z",
     "start_time": "2023-06-20T17:19:55.089626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   pclass  survived  sex      age  sibsp  parch      fare\n0     1.0         1    0  29.0000    0.0    0.0  211.3375\n1     1.0         1    1   0.9167    1.0    2.0  151.5500\n2     1.0         0    0   2.0000    1.0    2.0  151.5500\n3     1.0         0    1  30.0000    1.0    2.0  151.5500\n4     1.0         0    0  25.0000    1.0    2.0  151.5500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pclass</th>\n      <th>survived</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>211.3375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.9167</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>30.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.0000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>151.5500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = data.drop(['name', 'ticket', 'cabin', 'embarked'], axis=1) # remove unimportant columns\n",
    "\n",
    "le = preprocessing.LabelEncoder() # Create a label encoder\n",
    "le.fit(data['sex']) # provide data for it to learn what classes there are\n",
    "data['sex'] = le.transform(data['sex']) # apply the encoding\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-a-uiu0-LH3"
   },
   "source": [
    "In the above cell, we dropped a number of columns we don't suspect will be correlated with the target (*Note: we probably should have been a bit more rigorous about this!*). Then we used the `LabelEncoder()` within sklearn that can fit a numbering scheme to a categorical feature (i.e., 'Sex'). We can see in the new dataset, sex takes on a value of 0 (female) or 1 (male).\n",
    "\n",
    "##### Model Development\n",
    "\n",
    "OK! Let's get to developing some decision tree models to predict passenger survival. We will start with simple decision trees and develop more complex models from there. Our first step, as in previous labs, is to split our data into a training set and a test set (unseen data). We will then use k-folds cross validation on the training set to try and get the best performing model before finally applying it to the test data.\n",
    "\n",
    "Let's import sklearn's decision tree classifer and split the data (using techniques we covered in the first lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uXuxpDkP-LH3",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:19:56.167031Z",
     "start_time": "2023-06-20T17:19:56.024069Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree # import our decision tree model\n",
    "\n",
    "target_data = data[\"survived\"]\n",
    "feature_data = data.iloc[:, data.columns != \"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4U5WwH2-LH4"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* How many samples are in the training set? **916**\n",
    "* How many samples are in the test set? **393**\n",
    "* What are the survival rates in each of the datasets? **39%, 37%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ANVOauwm-LH4",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:20:16.179492Z",
     "start_time": "2023-06-20T17:20:16.175073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:          916\n",
      "Test set size:              393\n",
      "Training set survival rate: 0.39\n",
      "Test set survival rate:     0.37\n"
     ]
    }
   ],
   "source": [
    "## Your code here\n",
    "print(f'Training set size:          {X_train.shape[0]}')\n",
    "print(f'Test set size:              {X_test.shape[0]}')\n",
    "print(f'Training set survival rate: {y_train.sum()/y_train.shape[0]:.2f}')\n",
    "print(f'Test set survival rate:     {y_test.sum()/y_test.shape[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZp4c8Hg-LH5"
   },
   "source": [
    "##### Dealing with Missing Data: Imputation\n",
    "\n",
    "Before we can fit our decision tree to our training data, we can conduct *imputation* to replace missing values with the mean/median/mode value in the column. For this exercise we will conduct mode imputation (i.e., the most common value in the column).\n",
    "\n",
    "**YOUR TURN:** Assuming we have a feature vector with three rows where 'nan' is a missing value:\n",
    "X = [[1, 2, 3],\n",
    "     [1, 2, nan],\n",
    "     [2, 3, 2]]\n",
    "* Which sample has a missing value? (the 1st, 2nd or 3rd?) **2nd**\n",
    "* If we *impute* (i.e., replace the missing value with another value) using the mean (average), what value will go in place of the nan value? **2**\n",
    "\n",
    "It's important that you don't impute your data using statistics including the the test data! This is an example of *information leak* where your test data is leaking into your training data.\n",
    "\n",
    "As such, we will fit our missing data imputer to our training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fQFGk9hk-LH5",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:20:34.936491Z",
     "start_time": "2023-06-20T17:20:34.915572Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train) # replace missing data using our imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqj-hPeg-LH6"
   },
   "source": [
    "So we've got our data prepared, let's fit a decision tree to our training data.\n",
    "\n",
    "Remember, the pipeline for model development in sklearn is **initialize->fit->predict**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VqxAAXVU-LH6",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:20:37.176855Z",
     "start_time": "2023-06-20T17:20:37.165846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  97.37991266375546 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yMNmyr-LH6"
   },
   "source": [
    "In the above cell, we defined a Decision Tree classifier and fit it to our training set. When we then used it to predict training set values, the resulting accuracy was ~97%.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Since we are both training and predicting on our training set, why didn't the decision tree achieve 100% accuracy? (like our nearest neighbor approach with k=1)? **Decision trees are not guaranteed to be able to perfectly classify the training data because they are not guaranteed to be able to split the data into pure subsets.**\n",
    "* What is the performance of this model on the test set? **76%**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F4vftzMR-LH7",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:02.935769Z",
     "start_time": "2023-06-20T17:21:02.922238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  76.33587786259542 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Your code here\n",
    "X_test = imp.transform(X_test)\n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print (\"Accuracy: \", accuracy * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KZ5J-t-LH7"
   },
   "source": [
    "##### Feature Importances\n",
    "\n",
    "One thing we can do is take a look at the relative feature importances of the trained decision tree classifier. This will give us an idea of what the model thinks is more/less important for properly predicting the target.\n",
    "\n",
    "Let's look at the feature importances for a model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w-1ZcqEN-LH7",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:06.046083Z",
     "start_time": "2023-06-20T17:21:06.034046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare'], dtype='object')\n",
      "Relative feature importances:  [0.09748234 0.307769   0.22583609 0.04872318 0.01920284 0.30098656]\n"
     ]
    }
   ],
   "source": [
    "print (\"Features: \", feature_data.columns)\n",
    "print (\"Relative feature importances: \", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL_BERf-LH8"
   },
   "source": [
    "As we can see, the tree is placing a higher importance on Sex, Age, and Fare paid. These are interesting observations that we could dig a little deeper into if we wanted to.\n",
    "\n",
    "#### Visualizing the Tree\n",
    "\n",
    "One useful thing we can do is actually visualize our decision tree model! We can use the [graphViz](https://www.graphviz.org/) library to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ2xJfJm-LH8"
   },
   "outputs": [],
   "source": [
    "# from sklearn.tree import export_graphviz\n",
    "# import graphviz # Package containing visualization tools\n",
    "#\n",
    "# export_graphviz(clf, out_file=\"mytree.dot\", feature_names=data.columns.drop('survived')) # Save the visualization of the tree\n",
    "# with open(\"mytree.dot\") as f: # read the file back in\n",
    "#     dot_graph = f.read()\n",
    "# graphviz.Source(dot_graph) # display the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EgolEWB-LH8"
   },
   "source": [
    "**YOUR TURN:** Explore the decision tree and answer the following:\n",
    "* What feature does the root node split on?\n",
    "* What is the depth of the decision tree (i.e., the length of the longest path from root to leaf)?\n",
    "* Do you think this decision tree is prone to overfitting? Why/why not?\n",
    "\n",
    "To reduce the degree to which this tree is overfit to the training data, we can force the tree to be of some *maximum depth*. This ensures the tree won't be able to just keep generating new layers to properly classify every sample in the training stage (and, thus, presumably generalize better to the test set).\n",
    "\n",
    "Let's try limiting the max depth to 2 and visualizing the resulting tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4uDkSy0a-LH9",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:15.225114Z",
     "start_time": "2023-06-20T17:21:15.214117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(max_depth=2)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# export_graphviz(clf, out_file=\"mytree.dot\", feature_names=data.columns.drop('survived'))\n",
    "# with open(\"mytree.dot\") as f:\n",
    "#     dot_graph = f.read()\n",
    "# graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUJdXnp--LH9"
   },
   "source": [
    "Much simpler! As we can see, our model finds Age, Sex, and Pclass to be the most important features. We would expect this model to have much poorer performance when predicting on the training set (as opposed to our 97% we got above), but perhaps better performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2gqE8y3_yXP"
   },
   "source": [
    "There are many hyper-parameters that can be tuned to change how the model performs. Some common parameters that are modified include:\n",
    "1. Max Tree Depth: How \"tall\" do you want your tree to be\n",
    "2. Minimum Samples Per Leaf: This parameter defines the minimum number of training datapoints that fall into a given leaf node in order for that node to be created\n",
    "3. Minimum Samples to Split: This parameter controls the minimum number of samples required to create a decision split\n",
    "\n",
    "To decide the values of each of the parameters, we can use Grid Search combined with cross validation. In Grid Search, we first decide what potential values we want each hyperparameter will take. Then we find every possible combination of parameters and run cross validation on each combination to estimate the performance of that hyperparameter combination.\n",
    "\n",
    "Luckily, `sklearn` has a nice implementation of Grid Search that runs this algorithm for us. Lets see a demo below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zmpcYAMI_xyX",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:18.070883Z",
     "start_time": "2023-06-20T17:21:17.551478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV Accuracy:  81.54906153480637 %\n",
      "Testing Accuracy:  81.1704834605598 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda3/envs/bootcamp/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "clf = tree.DecisionTreeClassifier() # First we define our model without passing in parameters\n",
    "hyperparameter_search = { # Then we decide the possible parameter combinations\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 8, 11],\n",
    "    'min_samples_leaf': [2, 5, 8, 11]\n",
    "} # Since we have 3 parameters with 2 possible values, grid search will test 3^3 combinations\n",
    "evaluation_metric = make_scorer(accuracy_score, # GridSearchCV requires us to wrap our metric function in a \"scorer\"\n",
    "                                greater_is_better = True)\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = clf,\n",
    "                              param_grid = hyperparameter_search,\n",
    "                              scoring = evaluation_metric,\n",
    "                              cv = 5) # Set up search algorithm\n",
    "grid_search_cv.fit(X_train, y_train) # Run the search. NOTE: This may take a while\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_cv.best_params_) # Print the parameters\n",
    "print (\"Best CV Accuracy: \", grid_search_cv.best_score_ * 100, \"%\")\n",
    "\n",
    "clf = grid_search_cv.best_estimator_ # Get the best model from the GridSearch\n",
    "accuracy = accuracy_score(y_test, clf.predict(imp.transform(X_test)))\n",
    "print (\"Testing Accuracy: \", accuracy * 100, \"%\") # Print the testing accuracy of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qrr1hd59EiVK"
   },
   "source": [
    "In the cell above, we tested our two values per hyperparameter and ran grid search to find the best combination from the space we defined. As you may have noticed, the number of combinations tested by Grid Search exponentially increases as you test more values and tune more hyperparameters. This means that performing a grid search is often a task that takes a long period of time and is often note used for more complex models like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yotoft9s-LH9"
   },
   "source": [
    "### Random Forests, Gradient Boosting, Extreme Gradient Boosting\n",
    "\n",
    "Let's (briefly) investigate some more advanced tree models that you have learned about and see if we can improve our performance. We will be using the following models in addition to our decision tree classifier:\n",
    "\n",
    "* Scikitlearn Random Forest classifier\n",
    "* Scikitlearn Gradient boosting classifier\n",
    "* XGBoost classifier\n",
    "\n",
    "Note that XGBoost is similar in theory to Scikitlearn's Gradient boosting classifier. However, XGBoost's implementation is highly efficient than that of Scikitlearn's. Forests are *ensemble* techniques that combine multiple decision trees. As you learned in lecture, and as visualized below, random forest methods usually combine multiple trees through some sort of voting scheme.\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/random-forest.png?raw=1\" width=\"400\"/>\n",
    "\n",
    "In the above example, two trees vote (predict) Class B and another predicts Class A, so the overall ensemble vote goes to the majority (Class B). Boosting, on the other hand, uses multiple trees in in a stage-wise fashion. Popular machine learning software XGBoost has a great explanation for [how this works](https://xgboost.readthedocs.io/en/latest/tutorials/model.html).\n",
    "\n",
    "Let's build some ensemble classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kJHX6BYO-LH-",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:19.491855Z",
     "start_time": "2023-06-20T17:21:19.407438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-yptZhR-LH-"
   },
   "source": [
    "Now, let's see how they perform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6_8SgflL-LH-",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:22.398929Z",
     "start_time": "2023-06-20T17:21:20.801703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy: 0.78 (+/- 0.05)\n",
      "Random forest accuracy: 0.80 (+/- 0.06)\n",
      "Gradient boosting accuracy: 0.80 (+/- 0.05)\n",
      "XGBoost accuracy: 0.79 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_random = RandomForestClassifier()\n",
    "clf_gradient = GradientBoostingClassifier()\n",
    "clf_xgb = XGBClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "scores_random = cross_val_score(clf_random, X_train, y_train, cv=5)\n",
    "scores_gradient = cross_val_score(clf_gradient, X_train, y_train, cv=5)\n",
    "scores_xgb = cross_val_score(clf_xgb, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Decision tree accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Random forest accuracy: %0.2f (+/- %0.2f)\" % (scores_random.mean(), scores_random.std() * 2))\n",
    "print(\"Gradient boosting accuracy: %0.2f (+/- %0.2f)\" % (scores_gradient.mean(), scores_gradient.std() * 2))\n",
    "print(\"XGBoost accuracy: %0.2f (+/- %0.2f)\" % (scores_xgb.mean(), scores_xgb.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX8UnvyN-LH-"
   },
   "source": [
    "We can see that each of the more sophisticated tree/forest methods improves upon the initial decision tree accuracy in terms of cross-validated accuracy, with Extreme Gradient boosting providing the best result.\n",
    "\n",
    "Let's see how the extreme gradient boosted method performs on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ItWp3hBx-LH_",
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:22.475178Z",
     "start_time": "2023-06-20T17:21:22.396462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  80.1526717557252 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "imp.fit(X_test)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, clf_xgb.predict(X_test))\n",
    "\n",
    "print (\"Test set accuracy: \", accuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqpxFuwy-LH_"
   },
   "source": [
    "First, we imputed the missing values in the test set (as we had done for the training set) and then we applied our gradient boosting-based classifier (as trained on the training data). We yielded an 81% accuracy; not bad!\n",
    "\n",
    "**YOUR TURN:**\n",
    "* What features did the gradient boosting algorithm find the most important? **Sex and Pclass**\n",
    "* What is the test set accuracy if, instead, you used the Scikitlearn's gradient boosting algorithm? **80.92%**\n",
    "\n",
    "* If you designed a naive classifier that simply guessed 'did not survive' (i.e., Survived = 0) for every row in the test set, how would it perform? **62.60%**\n",
    "* Is this better or worse than our gradient boosted tree? **Worse**\n",
    "\n",
    "##### Congratulatons!\n",
    "\n",
    "You're finished this lab. On to the next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:57.252815Z",
     "start_time": "2023-06-20T17:21:57.237673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        Importance\nsex       0.610777\npclass    0.189595\nsibsp     0.091423\nfare      0.039600\nage       0.035536\nparch     0.033069",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sex</th>\n      <td>0.610777</td>\n    </tr>\n    <tr>\n      <th>pclass</th>\n      <td>0.189595</td>\n    </tr>\n    <tr>\n      <th>sibsp</th>\n      <td>0.091423</td>\n    </tr>\n    <tr>\n      <th>fare</th>\n      <td>0.039600</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.035536</td>\n    </tr>\n    <tr>\n      <th>parch</th>\n      <td>0.033069</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_dict = dict(zip(feature_data.columns, clf_xgb.feature_importances_))\n",
    "pd.DataFrame.from_dict(feature_importance_dict, orient='index', columns=['Importance']).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:57.534763Z",
     "start_time": "2023-06-20T17:21:57.411952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting accuracy: 80.92%\n"
     ]
    }
   ],
   "source": [
    "clf_gradient.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, clf_gradient.predict(X_test))\n",
    "print(f'Gradient Boosting accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T17:21:26.855212Z",
     "start_time": "2023-06-20T17:21:26.851515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier accuracy: 62.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy='constant', constant=0)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, dummy_clf.predict(X_test))\n",
    "print(f'Dummy classifier accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
