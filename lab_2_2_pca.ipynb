{"cells":[{"cell_type":"markdown","metadata":{"id":"XcK5U2gBGPUf"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2023/blob/main/lab_2_2_pca.ipynb)\n","\n","# UofT FASE ML Bootcamp\n","#### Tuesday June 20, 2023\n","#### Clustering and PCA - Lab 2, Day 2\n","#### Teaching team: Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya, Shehnaz Islam\n","##### Lab author: Alexander Olson, aolson@mie.utoronto.ca, edited by Jake Mosseri"]},{"cell_type":"markdown","metadata":{"id":"CPe6o2o3GPUj"},"source":["In this lab we will examine a number of clustering methods, starting with K-Means. We will also look at dimensionality reduction, covering two ways of visualizing high dimensional datasets. Both of these tasks are examples of _unsupervised learning_: they run without requiring labeled training data. Let's start with K-Means clustering."]},{"cell_type":"markdown","metadata":{"id":"7Ku8GGD-GPUk"},"source":["## K-Means Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8okplLlmGPUl"},"outputs":[],"source":["!pip install matplotlib\n","!pip install numpy\n","!pip install scikit-learn\n","!pip install scipy\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans"]},{"cell_type":"markdown","metadata":{"id":"tN7HkqxbGPUm"},"source":["The K-Means algorithm identifies a pre-determined number of clusters (K) within an unlabeled dataset. It does this using a simple procedure describing what each cluster should look like:\n","\n","* Each cluster can be represented by the mean (or centroid) of all the points in that cluster.\n","* Each point is closer to the mean point (centroid) in its own cluster than to that of any other cluster.\n","\n","Let's get a visual representation of what this looks like. First we will generate a simple 2D dataset containing four distinct blobs:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CBDWjbnGPUn"},"outputs":[],"source":["X, y_true = make_blobs(n_samples=300, centers=4,\n","                       cluster_std=0.60, random_state=0) # Create synthetic data\n","plt.scatter(X[:, 0], X[:, 1], s=50); # plot the synthetic data"]},{"cell_type":"markdown","metadata":{"id":"qjW1bEALGPUo"},"source":["It's pretty easy to tell where the four clusters are by eye, but let's see how K-Means performs at picking them out. We will also indicate the representative point for each cluster (the _cluster centre_)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRc3SwelGPUp"},"outputs":[],"source":["kmeans = KMeans(n_clusters=4) # create the KMeans model. Specify that we want 4 clusters\n","kmeans.fit(X) # run the clustering\n","y_kmeans = kmeans.predict(X) # get the cluster labels\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis') # plot the synthetic data colored by cluster\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5); # plot the cluster centers"]},{"cell_type":"markdown","metadata":{"id":"pu8MLZ7MGPUq"},"source":["The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly. Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to k-means involves an intuitive iterative approach known as _expectation–maximization_."]},{"cell_type":"markdown","metadata":{"id":"2nOig1lxGPUr"},"source":["## Expectation Maximization\n","\n","Expectation Maximization (EM) is a powerful algorithm that comes up a lot in different avenues of data science. K-Means is a good example of this algorithm, and we'll go through it briefly here. In short, K-Means consists of the following procedure:\n","\n","* Guess some cluster centres (at random, or using a heuristic)\n","* Repeat until convergence (i.e. the clusters stop changing between steps):\n","    * _E-Step_: Assign each point to the closest cluster centre\n","    * _M-Step_: Set the cluster centres to the mean\n","\n","Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to. The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n","\n","The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.\n","\n","The k-Means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1-5nzKuGPUs"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances_argmin\n","\n","def find_clusters(X, n_clusters, rseed=2):\n","    # 1. Randomly choose clusters\n","    rng = np.random.RandomState(rseed)\n","    i = rng.permutation(X.shape[0])[:n_clusters]\n","    centers = X[i]\n","\n","    while True:\n","        # 2a. Assign labels based on closest center\n","        labels = pairwise_distances_argmin(X, centers)\n","\n","        # 2b. Find new centers from means of points\n","        new_centers = np.array([X[labels == i].mean(0)\n","                                for i in range(n_clusters)])\n","\n","        # 2c. Check for convergence\n","        if np.all(centers == new_centers):\n","            break\n","\n","        centers = new_centers\n","        plt.scatter(X[:, 0], X[:, 1], c=labels,\n","                    s=50, cmap='viridis');\n","        plt.pause(0.05)\n","    return centers, labels\n","\n","centers, labels = find_clusters(X, 4)\n"]},{"cell_type":"markdown","metadata":{"id":"X0fWky2dGPUt"},"source":["## Caveats\n","\n","If K-Means were perfect, we could stop here. But unfortunately, while elegant, this algorithm does come with its own set of problems. For example, the random seed used to initialize the clusters can affect the results - there is no guarantee that the algorithm will find the best _possible_ solution. (For an example of this, try changing the random seed in the previous example to `3`).\n","\n","Another common problem with K-Means is that you must tell it how many clusters to look for - it cannot learn this from the data. This is fine in simple, low-dimensional examples such as the one above, but when the data is high-dimensional it becomes impossible to eyeball the correct number of clusters for your data. In fact, we can pass any number of clusters to the algorithm and it won't complain:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ny8EfiW4GPUt"},"outputs":[],"source":["kmeans = KMeans(n_clusters=50)\n","kmeans.fit(X)\n","y_kmeans = kmeans.predict(X)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=25, cmap='nipy_spectral')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"fV4Cm6PCGPUu"},"source":["Again, this is obviously wrong in this example, but the algorithm has no way of knowing this. However, there *is* a way to identify the best value for K from the data - at least, heuristically."]},{"cell_type":"markdown","metadata":{"id":"gH1k3zECGPUu"},"source":["## Finding the Elbow"]},{"cell_type":"markdown","metadata":{"id":"qAI6Qh1eGPUu"},"source":["If you were tasked with calculating a score that indicates the best possible fit for the data, you might try the average distance for each point in a cluster to its cluster centre. The logic here would be that a better cluster should have all its points close to its centre - hence, the descriptive power of that cluster centre would be higher.\n","\n","Unfortunately, you may realize, the more clusters there are, the better this score would be. In fact, for N data points, the optimal value would be N=K! At this value, every point would be exactly its cluster centre. But then we aren't really clustering at all!\n","\n","The point is, as K increases, the average proximity to the cluster centre is _guaranteed_ to increase. But there is still some information we can gain from this metric. While the value itself always increases, the _rate of change_ can differ - and here is the secret to identifying the best value for K."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pm_uMcLBGPUv"},"outputs":[],"source":["from scipy.spatial.distance import cdist\n","\n","av_dist = []\n","\n","for k in range(1,10):\n","    km = KMeans(n_clusters=k).fit(X)\n","    av_dist.append(\n","        sum(\n","            np.min(\n","                cdist(X, km.cluster_centers_, 'euclidean'),\n","                axis=1\n","            )\n","        ) / X.shape[0]\n","    )\n","\n","plt.plot(range(1,10), av_dist)\n","plt.xlabel('Value for K')\n","plt.ylabel('Average distance to cluster centre');"]},{"cell_type":"markdown","metadata":{"id":"KsIwdxXTGPUv"},"source":["Looking at this graph, it's transparently obvious where the improvement with a higher K stops becoming substantial - K = 4, just as we would expect. If we plot the _improvement_, this transition becomes even more obvious:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jNqDL6SGPUw"},"outputs":[],"source":["plt.plot(range(1,10),-np.gradient(av_dist))\n","plt.xlabel('Value for K')\n","plt.ylabel('Improvement from previous K');"]},{"cell_type":"markdown","metadata":{"id":"U4KjCinpGPUw"},"source":["The _elbow_ is the point in the line where improvement transitions from substantial to minimal. Of course, such a clear elbow is not guaranteed - if your dataset doesn't cluster very well, it might be the case that there's no clear transition from good improvement to poor improvement. But in situations where there *is* a good value for K waiting to be found, the Elbow method can help us consistently find it.\n","\n","Now, you try on the Iris dataset from Lab 1. This data is 4-dimensional, so we can't plot it to quickly find the number of clusters. Instead, we need the Elbow. First, calculate the average distance to cluster centre for a range of K values, then identify the elbow."]},{"cell_type":"markdown","metadata":{"id":"zIpjzu0HGPUx"},"source":["**YOUR TURN**\n","* Using the Iris dataset, and the Elbow technique, identify the best number of clusters and plot a graph showing the elbow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJg80R6tGPUw"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","iris_data = load_iris() # Load the Iris Data\n","feature_data = iris_data.data # get the features from the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztsVdz2gGPUx"},"outputs":[],"source":["X = feature_data\n","\n","av_dist = []\n","\n","#Insert your code here to identify the best number of clusters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Sv7yE2SGPUx"},"outputs":[],"source":["#Insert your code here to plot a graph showing the elbow"]},{"cell_type":"markdown","metadata":{"id":"fGV7ndAUGPUy"},"source":["### Dimensionality reduction and principle component analysis\n","\n","As you get more in-depth in the world of data science, you'll learn that in practice it's very uncommon to work with datasets that are 2 or 3 dimensional, and so can be plotted directly. We're now going to look at _dimensionality reduction_: a category of unsupervised algorithms which attempt to collapse high-dimensional datasets into a low-dimensional space.\n","\n","Obviously, as suggested above, one reason to do this is to aid visualization. But that's far from the only reason dimensionality reduction is useful! These techniques also allow us to filter noise, extract useful features, and much more.\n","\n","Let's dive into PCA. We'll first take a two dimensional dataset and reduce it to one dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2LCs_PESGPUy"},"outputs":[],"source":["rng = np.random.RandomState(1) # set the random state for reproducability\n","\n","X = np.dot(rng.rand(2,2),\n","          rng.randn(2,200)).T # create synthetic data\n","plt.scatter(X[:,0],X[:,1]) # plot synthetic data\n","plt.axis('equal');"]},{"cell_type":"markdown","metadata":{"id":"DNnI-cPmGPUy"},"source":["Here we have a two dimensional dataset that, by eye, has a nearly linear relationship between the X and Y values. This is reminiscent of the type of data we would look to explore with linear regression, but the problem setting here is slightly different: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values.\n","\n","In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's PCA estimator, we can compute this as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4M5vDm-mGPUy"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","pca.fit(X);"]},{"cell_type":"markdown","metadata":{"id":"5kExbKbkGPUz"},"source":["The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1YDvyu2GPUz"},"outputs":[],"source":["pca.components_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSBVHdnnGPUz"},"outputs":[],"source":["pca.explained_variance_"]},{"cell_type":"markdown","metadata":{"id":"Esw1b3ltGPU0"},"source":["To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_T6MNK-GPU0"},"outputs":[],"source":["def draw_vector(v0, v1, ax=None):\n","    ax = ax or plt.gca()\n","    arrowprops=dict(arrowstyle='->',\n","                    linewidth=2,\n","                    shrinkA=0, shrinkB=0, color='black')\n","    ax.annotate('', v1, v0, arrowprops=arrowprops)\n","\n","# plot data\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","for length, vector in zip(pca.explained_variance_, pca.components_):\n","    v = vector * 3 * np.sqrt(length)\n","    draw_vector(pca.mean_, pca.mean_ + v)\n","plt.axis('equal');"]},{"cell_type":"markdown","metadata":{"id":"Rf9Gvj0UGPU0"},"source":["These vectors represent the principal axes of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the \"principal components\" of the data.\n","\n","You have likely noticed that in this example, almost all of the variance is explained by the first axis, while the second axis explains very little variance. This is where PCA becomes a tool for dimensionality reduction. By transforming the data to lie along the PCA dimensions and then only keeping the first K dimensions, we can reduce our dataset into K dimensional space:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdg0VebdGPU0"},"outputs":[],"source":["pca = PCA(n_components=1)\n","pca.fit(X)\n","X_pca = pca.transform(X)\n","print(\"original shape:   \", X.shape)\n","print(\"transformed shape:\", X_pca.shape)\n","plt.hist(X_pca,bins=20)\n","plt.xlabel('Value in reduced space')\n","plt.ylabel('Count');"]},{"cell_type":"markdown","metadata":{"id":"jzw1cDaUGPU1"},"source":["If we project our new, low-dimensional data back into the original space, we can get a good visual intuition of what PCA has done for us:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHWQrAXEGPU1"},"outputs":[],"source":["X_new = pca.inverse_transform(X_pca)\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n","plt.axis('equal');"]},{"cell_type":"markdown","metadata":{"id":"PVfR41wnGPU1"},"source":["The light points are the original data, while the orange points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n","\n","This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.\n","\n","The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data. To see this, let's take a quick look at the application of PCA to a much higher dimensional dataset.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKAM_YoFGPU2"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.keys()\n","# set up the figure\n","fig = plt.figure(figsize=(6, 6))  # figure size in inches\n","fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n","\n","# plot the digits: each image is 8x8 pixels\n","for i in range(64):\n","    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n","    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n","\n","    # label the image with the target value\n","    ax.text(0, 7, str(digits.target[i]))"]},{"cell_type":"markdown","metadata":{"id":"1lNM9-MUGPU2"},"source":["This dataset contains a series of hand-written digits, translated to an 8x8 (i.e. 64 dimensional) grid. To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1yhLKN0GPU2"},"outputs":[],"source":["pca = PCA(2)  # project from 64 to 2 dimensions\n","projected = pca.fit_transform(digits.data) # this is a quicker way to call pca.fit(X) and then pca.transform(X)\n","print(digits.data.shape)\n","print(projected.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXwTOVaEGPU2"},"outputs":[],"source":["plt.scatter(projected[:, 0], projected[:, 1],\n","            c=digits.target, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"p9zF9ZoPGPU3"},"source":["Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels."]},{"cell_type":"markdown","metadata":{"id":"08Ueac5TGPU3"},"source":["**YOUR TURN**\n","\n","Load the wine dataset from lab 1-2 and use PCA to reduce its dimensionality to two dimensions. Then, plot the dataset and color each point according to its label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZAfhRHeGPU3"},"outputs":[],"source":["from sklearn.datasets import load_wine\n","\n","wine_data = load_wine()\n","\n","feature_data = wine_data.data\n","\n","#Insert your code here"]},{"cell_type":"markdown","metadata":{"id":"j3K5lsV8GPU3"},"source":["## TSNE\n","\n","We are now going to look at a different method of dimensionality reduction: t-SNE, a method partially developed here at U of T by Geoffrey Hinton in 2008. It differs from PCA in that it uses the _local relationships_ between points to create a low-dimensional mapping. Among other things, this allows t-SNE to capture non-linear structures in the data.\n","\n","There are a number of other benefits to t-SNE over PCA, but since we are focusing on using dimensionality reduction for visualization we will stick to the benefits which are apparent in that space. Because t-SNE considers the local relationships between points, it ensures that the distances between points in the low dimensional mapping are representative of the distances in the original space. This makes it a lot more useful for visualization compared to PCA!"]},{"cell_type":"markdown","metadata":{"id":"KyHXxVcuGPU3"},"source":["### How t-SNE works"]},{"cell_type":"markdown","metadata":{"id":"fIwBl1DJGPU4"},"source":["t-SNE – at a high level – basically works like this:\n","\n","Step 1: In the high-dimensional space, create a probability distribution that dictates the relationships between various neighboring points\n","\n","Step 2: It then tries to recreate a low dimensional space that follows that probability distribution as best as possible.\n","\n","The “t” in t-SNE comes from the t-distribution, which is the distribution used in Step 2. The “S” and “N” (“stochastic” and “neighbor”) come from the fact that it uses a probability distribution across neighboring points.\n","\n","Let's look at a more advanced version of the handwritten digits dataset to get a visual sense of how t-SNE and PCA differ. The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset comprises 70,000 handwritten digits taken from the American Census Bureau and labelled by American high school students. It's a benchmark of learning about ML, and as such it's easy to download:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMuzJqwIGPU4"},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","mnist = fetch_openml('mnist_784',cache=False) #This could take a little while - it's a lot of data.\n","X = mnist['data']\n","y = mnist['target']\n","y = [int(t) for t in y]\n","X.shape"]},{"cell_type":"markdown","metadata":{"id":"9i2HAFp-GPU4"},"source":["First, we use our knowledge of PCA to reduce MNIST to two dimensions and plot it, color coded by target value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjjE2pdmGPU4"},"outputs":[],"source":["plt.figure(figsize=(16,10));\n","\n","pca = PCA(2)  # project from 64 to 2 dimensions\n","projected = pca.fit_transform(X)\n","\n","plt.scatter(projected[:, 0], projected[:, 1],\n","            c=y, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"VrbFZCnUGPU4"},"source":["Okay, so that should be similar to what we saw above with the smaller digit dataset, but it's honestly not very useful. Everything is just in a huge, formless blob and we aren't really gathering much information - it's just overwhelming. So let's see what t-SNE can do.\n","\n","**NOTE** t-SNE is going to take a lot longer to run than PCA. Although you *can* run it on the entire 70,000 image dataset, this could take a significant amount of time even on a high end computer. If you don't want to wait around that long, uncomment the line below to get a random subsample of the data which you can use instead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTrNCTonGPU5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","#_,X,_,y = train_test_split(X,y,test_size=1/14)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MbBPUsqBGPU5"},"outputs":[],"source":["import time\n","from sklearn.manifold import TSNE\n","\n","time_start = time.time()\n","tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n","tsne_results = tsne.fit_transform(X)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xF4LBI-wGPU5"},"outputs":[],"source":["plt.figure(figsize=(16,10))\n","plt.scatter(tsne_results[:, 0], tsne_results[:, 1],\n","            c=y, edgecolor='none', alpha=0.5,\n","            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n","plt.xlabel('component 1')\n","plt.ylabel('component 2')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"nHBmgSbIGPU5"},"source":["Woah! Using t-SNE, the clusters are much more clearly defined, with different numbers being clearly separated in the low dimensional space. Running a clustering algorithm like K-Means now would likely give us pretty reasonable performance on classifying the digits, even just from two dimensions.\n","\n","Congratulations! You're finished with this lab. For a more detailed introduction to t-SNE which dives into the math behind it, take a look [here](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}